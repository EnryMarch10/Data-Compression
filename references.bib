@article{OnTheRandomnessOfCompressedData,
  author         = {Klein, Shmuel T. and Shapira, Dana},
  title          = {On the Randomness of Compressed Data},
  journal        = {Information},
  volume         = {11},
  year           = {2020},
  number         = {4},
  article-number = {196},
  URL            = {https://www.mdpi.com/2078-2489/11/4/196},
  ISSN           = {2078-2489},
  abstract       = {It seems reasonable to expect from a good compression method that its output should not be further compressible, because it should behave essentially like random data. We investigate this premise for a variety of known lossless compression techniques, and find that, surprisingly, there is much variability in the randomness, depending on the chosen method. Arithmetic coding seems to produce perfectly random output, whereas that of Huffman or Ziv-Lempel coding still contains many dependencies. In particular, the output of Huffman coding has already been proven to be random under certain conditions, and we present evidence here that arithmetic coding may produce an output that is identical to that of Huffman.},
  DOI            = {10.3390/info11040196}
}

@article{EntropyAndInformationTheoryUsesAndMisuses,
  author         = {Ben-Naim, Arieh},
  title          = {Entropy and Information Theory: Uses and Misuses},
  journal        = {Entropy},
  volume         = {21},
  year           = {2019},
  number         = {12},
  article-number = {1170},
  URL            = {https://www.mdpi.com/1099-4300/21/12/1170},
  ISSN           = {1099-4300},
  abstract       = {This article is about the profound misuses, misunderstanding, misinterpretations and misapplications of entropy, the Second Law of Thermodynamics and Information Theory. It is the story of the “Greatest Blunder Ever in the History of Science”. It is not about a single blunder admitted by a single person (e.g., Albert Einstein allegedly said in connection with the cosmological constant, that this was his greatest blunder), but rather a blunder of gargantuan proportions whose claws have permeated all branches of science; from thermodynamics, cosmology, biology, psychology, sociology and much more.},
  DOI            = {10.3390/e21121170}
}

@book{AlgorithmicRandomnessAndComplexity,
  author    = {Downey, Rodney G. and Hirschfeldt, Denis R.},
  title     = {Algorithmic Randomness and Complexity},
  year      = {2010},
  publisher = {Springer},
  keywords  = {randomness},
  URL       = {https://books.google.it/books?id=FwIKhn4RYzYC&lpg=PR3&ots=_nbPVe6IUi&dq=randomness%20in%20computer%20science&lr&pg=PA226#v=onepage&q&f=false}
}

@article{AlgorithmicInformationTheoryBriefGuide,
  title   = {Algorithmic information theory: a brief non-technical guide to the field},
  author  = {Hutter, Marcus},
  journal = {arXiv preprint cs/0703024},
  year    = {2007}
}

@book{AConciseIntroductionToDataCompression,
  title     = {A Concise Introduction to Data Compression},
  author    = {Salomon, D.},
  isbn      = {9781848000728},
  lccn      = {2007939563},
  series    = {Undergraduate Topics in Computer Science},
  URL       = {https://books.google.it/books?id=mnpeizY0btYC},
  year      = {2007},
  publisher = {Springer London}
}

@article{AnIntroductionToInformationTheoryAndEntropy,
  title   = {An introduction to information theory and entropy},
  author  = {Carter, Tom},
  journal = {Complex systems summer school, Santa Fe},
  year    = {2007}
}

@article{IntroductionToDataCompression,
  title   = {Introduction to data compression},
  author  = {Blelloch, Guy E and others},
  journal = {Computer Science Department, Carnegie Mellon University},
  volume  = {54},
  year    = {2001}
}

@article{TheDataCompressionBook2ndEdition,
  title   = {The data compression book 2nd edition},
  author  = {Nelson, Mark and Gailly, Jean-Loup},
  journal = {M \& T Books, New York, NY},
  year    = {1995}
}

@article{ThreeApproachesToTheQuantitativeDefinitionOfInformation,
  author    = {Kolmogorov, A. N.},
  title     = {Three approaches to the quantitative definition of information},
  journal   = {International Journal of Computer Mathematics},
  volume    = {2},
  number    = {1-4},
  pages     = {157--168},
  year      = {1968},
  publisher = {Taylor \& Francis},
  doi       = {10.1080/00207166808803030},
  URL       = {https://doi.org/10.1080/00207166808803030},
  eprint    = {https://doi.org/10.1080/00207166808803030}
}
% https://knowen-production.s3.amazonaws.com/uploads/attachment/file/3520/Three%2Bapproaches%2Bto%2Bthe%2Bquantitative%2Bdefinition%2Bof%2Binformation.pdf

@article{TheDefinitionOfRandomSequences,
  title    = {The definition of random sequences},
  journal  = {Information and Control},
  volume   = {9},
  number   = {6},
  pages    = {602-619},
  year     = {1966},
  ISSN     = {0019-9958},
  DOI      = {https://doi.org/10.1016/S0019-9958(66)80018-9},
  URL      = {https://www.sciencedirect.com/science/article/pii/S0019995866800189},
  author   = {Martin-Löf, Per},
  abstract = {Kolmogorov has defined the conditional complexity of an object y when the object x is already given to us as the minimal length of a binary program which by means of x computes y on a certain asymptotically optimal machine. On the basis of this definition he has proposed to consider those elements of a given large finite population to be random whose complexity is maximal. Almost all elements of the population have a complexity which is close to the maximal value. In this paper it is shown that the random elements as defined by Kolmogorov possess all conceivable statistical properties of randomness. They can equivalently be considered as the elements which withstand a certain universal stochasticity test. The definition is extended to infinite binary sequences and it is shown that the non random sequences form a maximal constructive null set. Finally, the Kollektivs introduced by von Alises obtain a definition which seems to satisfy all intuitive requirements.}
}

@article{AMathematicalTheoryOfCommunication,
  author   = {Shannon, C. E.},
  journal  = {The Bell System Technical Journal}, 
  title    = {A mathematical theory of communication}, 
  year     = {1948},
  volume   = {27},
  number   = {3},
  pages    = {379-423},
  keywords = {},
  DOI      = {10.1002/j.1538-7305.1948.tb01338.x}
}
